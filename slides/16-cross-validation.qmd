---
title: "Cross validation"
author: "Prof. Maria Tackett"
date: "2022-10-24"
date-format: "MMM DD, YYYY"
footer: "[ðŸ”— Week 09](https://sta210-fa22.netlify.app/weeks/week-09.html)"
logo: "../images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: true
    incremental: false 
    chalkboard: true
execute:
  freeze: auto
  echo: true
  warning: false
  message: false
knitr:
  opts_chunk: 
    R.options:      
    width: 200
---

```{r}
#| include: false

# figure options
knitr::opts_chunk$set(
  fig.width = 10, fig.asp = 0.618, out.width = "90%",
  fig.retina = 3, dpi = 300, fig.align = "center"
)
```

## Announcements

-   [Lab 05](https://sta210-fa22.netlify.app/labs/lab-05.html)

    -   due TODAY, 11:59pm (Thu labs)

    -   due Tuesday, 11:59pm (Fri labs)

-   Click here for [Week 09](https://sta210-fa22.netlify.app/weeks/week-09.html) activities.

## Topics

::: nonincremental
-   Cross validation for model evaluation
-   Cross validation for model comparison
:::

## Computational setup

```{r}
#| echo: true

# load packages
library(tidyverse)
library(tidymodels)
library(knitr)
library(schrute)
```

## Data & goal {.smaller}

::: nonincremental
-   Data: The data come from the [**shrute**](https://bradlindblad.github.io/schrute/) package, and has been transformed using instructions from [Lab 04](../labs/lab-04.html)
-   Goal: Predict `imdb_rating` from other variables in the dataset
:::

```{r}
#| echo: true

office_episodes <- read_csv(here::here("slides/data/office_episodes.csv"))
office_episodes
```

# Modeling prep

## Split data into training and testing

```{r}
set.seed(123)
office_split <- initial_split(office_episodes)
office_train <- training(office_split)
office_test <- testing(office_split)
```

## Specify model

```{r}
office_spec <- linear_reg() |>
  set_engine("lm")

office_spec
```

# Model 1

## From Lab 04

Create a recipe that uses the newly generated variables

-   Denotes `episode_name` as an ID variable and doesn't use `air_date` or `season` as predictors
-   Create dummy variables for all nominal predictors
-   Remove all zero variance predictors

## Create recipe

```{r}
office_rec1 <- recipe(imdb_rating ~ ., data = office_train) |>
  update_role(episode_name, new_role = "id") |>
  step_rm(air_date, season) |>
  step_dummy(all_nominal_predictors()) |>
  step_zv(all_predictors())

office_rec1
```

## Preview recipe

```{r}
prep(office_rec1) |>
  bake(office_train) |>
  glimpse()
```

## Create workflow

```{r}
office_wflow1 <- workflow() |>
  add_model(office_spec) |>
  add_recipe(office_rec1)

office_wflow1
```

## Fit model to training data

. . .

*Not so fast!*

# Cross validation

## Spending our data

-   We have already established that the idea of data spending where the test set was recommended for obtaining an unbiased estimate of performance.
-   However, we usually need to understand the effectiveness of the model [*before*]{.underline} *using the test set*.
-   Typically we can't decide on *which* final model to take to the test set without making model assessments.
-   **Remedy:** Resampling to make model assessments on training data in a way that can generalize to new data.

## Resampling for model assessment

**Resampling is only conducted on the** <u>**training**</u> **set**. The test set is not involved. For each iteration of resampling, the data are partitioned into two subsamples:

-   The model is fit with the **analysis set**. Model fit statistics such as $R^2_{Adj}$, AIC, and BIC are calculated based on this fit.
-   The model is evaluated with the **assessment set**.

## Resampling for model assessment

![](images/16/resampling.svg){fig-align="center"}

<br>

Source: Kuhn and Silge. [Tidy modeling with R](https://www.tmwr.org/).

## Analysis and assessment sets

-   Analysis set is analogous to training set.
-   Assessment set is analogous to test set.
-   The terms *analysis* and *assessment* avoids confusion with initial split of the data.
-   These data sets are mutually exclusive.

## Cross validation

More specifically, **v-fold cross validation** -- commonly used resampling technique:

-   Randomly split your **training** **data** into *v* partitions
-   Use *v-1* partitions for analysis, and the remaining 1 partition for analysis (model fit + model fit statistics)
-   Repeat *v* times, updating which partition is used for assessment each time

. . .

Let's give an example where `v = 3`...

## Cross validation, step 1

Randomly split your **training** **data** into 3 partitions:

<br>

![](images/16/three-CV.svg){fig-align="center"}

## Split data

```{r}
#| echo: true

set.seed(345)
folds <- vfold_cv(office_train, v = 3)
folds
```

## Cross validation, steps 2 and 3

::: nonincremental
-   Use *v-1* partitions for analysis, and the remaining 1 partition for assessment
-   Repeat *v* times, updating which partition is used for assessment each time
:::

![](images/16/three-CV-iter.svg){fig-align="center"}

## Fit resamples {.midi}

```{r}
#| echo: true

# Function to get Adj R-sq, AIC, BIC
calc_model_stats <- function(x) {
  glance(extract_fit_parsnip(x)) |>
    select(adj.r.squared, AIC, BIC)
}

set.seed(456)

# Fit model and calculate statistics for each fold
office_fit_rs1 <- office_wflow1 |>
  fit_resamples(resamples = folds, 
                control = control_resamples(extract = calc_model_stats))

office_fit_rs1
```

## Cross validation, now what?

-   We've fit a bunch of models
-   Now it's time to use them to collect metrics (e.g., \$R\^2\$, AIC, RMSE, etc. ) on each model and use them to evaluate model fit and how it varies across folds

## Collect $R^2$ and RMSE from CV 

```{r}
# Produces summary across all CV
collect_metrics(office_fit_rs1)
```

<br>

Note: These are calculated using the *assessment* data

## Deeper look into $R^2$ and RMSE 

```{r}
cv_metrics1 <- collect_metrics(office_fit_rs1, summarize = FALSE) 

cv_metrics1
```

## Better tabulation of $R^2$ and RMSE from CV 

```{r}
cv_metrics1 |>
  mutate(.estimate = round(.estimate, 3)) |>
  pivot_wider(id_cols = id, names_from = .metric, values_from = .estimate) |>
  kable(col.names = c("Fold", "RMSE", "R-squared"))
```

## How does RMSE compare to y? {.small}

::: columns
::: {.column width="50%"}
Cross validation RMSE stats:

```{r}
cv_metrics1 |>
  filter(.metric == "rmse") |>
  summarise(
    min = min(.estimate),
    max = max(.estimate),
    mean = mean(.estimate),
    sd = sd(.estimate)
  )
```
:::

::: {.column width="50%"}
Training data IMDB score stats:

```{r}
office_episodes |>
  
  summarise(
    min = min(imdb_rating),
    max = max(imdb_rating),
    mean = mean(imdb_rating),
    sd = sd(imdb_rating)
  )
```
:::
:::

## Collect $R^2_{Adj}$, AIC, BIC from CV

```{r}
map_df(office_fit_rs1$.extracts, ~ .x[[1]][[1]]) |>
  bind_cols(Fold = office_fit_rs1$id)
```

<br>

Note: These are based on the model fit from the *analysis* data

## Cross validation jargon

-   Referred to as *v-fold* or *k-fold* cross validation
-   Also commonly abbreviated as CV

## Cross validation in practice

::: incremental
-   To illustrate how CV works, we used `v = 3`:

    ::: nonincremental
    -   Analysis sets are 2/3 of the training set
    -   Each assessment set is a distinct 1/3
    -   The final resampling estimate of performance averages each of the 3 replicates
    :::

-   This was useful for illustrative purposes, but `v = 3` is a poor choice in practice

-   Values of `v` are most often 5 or 10; we generally prefer 10-fold cross-validation as a default
:::

# Application exercise

::: appex
ðŸ“‹ [AE 10: Cross validation](../ae/ae-10-cross-validation.html)
:::

## Recap

::: nonincremental
-   Cross validation for model evaluation
-   Cross validation for model comparison
:::
